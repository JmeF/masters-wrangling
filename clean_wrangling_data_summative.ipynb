{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling data summative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import bs4 as bs\n",
    "import pprint as pp\n",
    "import pandas as pd \n",
    "import re\n",
    "import mwparserfromhell as mwp\n",
    "\n",
    "import tldextract\n",
    "import urllib.request\n",
    "\n",
    "#### Part 1: Get the data's\n",
    "\n",
    "# Notes: \n",
    "# - Use page id in case countries change name (Czechia)\n",
    "# - (Nuisance of the XML tag called text)\n",
    "# - Keep two years separate\n",
    "# - keep \"/n\" returns in -- because \n",
    "\n",
    "# Function to get XML from files, iterate through pages and pull out id,title (country name) \n",
    "# and text into a pandas DataFrame object\n",
    "\n",
    "def makeDF(date):\n",
    "    wikitext = open(\"WD_wikipediaCountries_{}.xml\".format(date),\"rb\").read().decode(\"utf-8\")   \n",
    "    wikisoup = bs.BeautifulSoup(wikitext, \"lxml\")\n",
    "    rows = []\n",
    "    for c,i in enumerate(wikisoup.findAll(\"page\")):\n",
    "        newdic = {}                                            \n",
    "        newdic[\"country\"] = i.title.text\n",
    "        newdic[\"text{}\".format(date[4:])] = i.find(\"text\").text\n",
    "        temp_df  = pd.DataFrame([newdic.values()],index=[i.id.text] ,columns=newdic.keys())\n",
    "        rows.append(temp_df)\n",
    "    wiki_df = pd.concat(rows,axis=0)\n",
    "    return wiki_df\n",
    "\n",
    "df_2009 = makeDF(\"01012009\")\n",
    "df_2019 = makeDF(\"07012019\")\n",
    "\n",
    "mergedFrame = df_2009.merge(df_2019, left_index=True,right_index=True)\n",
    "\n",
    "#Check nothing has gone wrong - that both columns are aligned\n",
    "display(mergedFrame[mergedFrame[\"country_x\"] != mergedFrame[\"country_y\"]])\n",
    "\n",
    "#delete \"country_y\" and rename \"country_x\" to country\n",
    "mergedFrame = mergedFrame.drop(\"country_y\",axis=1)\n",
    "mergedFrame.columns = [\"country\",\"text2009\",\"text2019\"]\n",
    "\n",
    "bothyears = [\"2009\",\"2019\"] \n",
    "\n",
    "#### Part 2 - cleaning\n",
    "\n",
    "### Part 2.1 - Extracting links\n",
    "\n",
    "## extract the links from a text\n",
    "def what_links(dirty_text, year, istest=False):\n",
    "    ustlds = [\"gov\",\"edu\",\"mil\"]\n",
    "    if istest == True:\n",
    "        #Validating the external link extraction\n",
    "        ext_link_regex = re.compile(r'https?://[\\w\\./?&=%]*')\n",
    "        ext_links = ext_link_regex.findall(dirty_text)\n",
    "    else:\n",
    "        wikipage = mwp.parse(dirty_text,skip_style_tags=True)\n",
    "        ext_links = wikipage.filter_external_links()  \n",
    "    \n",
    "    counts = {\"us_count{}\".format(year):0,\"other_count{}\".format(year):0}\n",
    "    us_suffixes = []\n",
    "    other_suffixes = []\n",
    "    for link in ext_links:\n",
    "        if istest == True:\n",
    "            url = link\n",
    "        else:\n",
    "            url = link.split(\" \",1)[0]\n",
    "            url = url.replace(\"[\",\"\")\n",
    "            url = url.replace(\"]\",\"\")\n",
    "        suffix = tldextract.extract(url).suffix\n",
    "        if suffix in ustlds:\n",
    "            counts[\"us_count{}\".format(year)] += 1\n",
    "            us_suffixes.append(suffix)\n",
    "        else:\n",
    "            counts[\"other_count{}\".format(year)] += 1\n",
    "            other_suffixes.append(suffix)\n",
    "    counts[\"us_suffixes{}\".format(year)] = set(us_suffixes)\n",
    "    counts[\"other_suffixes{}\".format(year)] = set(other_suffixes)\n",
    "    return counts\n",
    "\n",
    "## extract links from a dataframe\n",
    "def getLinks(df, years = bothyears,istest = False):\n",
    "    if istest == True:\n",
    "        test = \"test\"\n",
    "    else:\n",
    "        test=\"\"\n",
    "    for year in years:\n",
    "        df[\"what_links{}{}\".format(year,test)] = df.apply(lambda x: what_links(x[\"text{}\".format(year)],year,istest=istest),axis=1)\n",
    "        #Unpack the columns\n",
    "        link_values = {\"links_us{}{}\".format(year,test):'us_count{}'.format(year),\n",
    "                       \"links_other{}{}\".format(year,test):'other_count{}'.format(year),\n",
    "                       \"links_suffix_us{}{}\".format(year,test):'us_suffixes{}'.format(year),\n",
    "                       \"links_suffix_other{}{}\".format(year,test):'other_suffixes{}'.format(year)}\n",
    "        for col_name,dic_name in link_values.items():\n",
    "            df[col_name] = df[\"what_links{}{}\".format(year,test)].map(lambda x: x[dic_name])\n",
    "        del df[\"what_links{}{}\".format(year,test)]\n",
    "    return df\n",
    "\n",
    "#Run the function\n",
    "mergedFrame = getLinks(mergedFrame)\n",
    "\n",
    "for year in bothyears:\n",
    "    mergedFrame[\"links_total{}\".format(year)] = mergedFrame.apply(lambda x: x[\"links_own{}\".format(year)] + x[\"links_us{}\".format(year)] + x[\"links_other{}\".format(year)],axis=1)\n",
    "\n",
    "## Tests on regex vs mwp\n",
    "mergedFrame = getLinks(mergedFrame, istest=True)\n",
    "\n",
    "mean_2009_own = (mergedFrame[\"links_own2009\"] - mergedFrame[\"links_own2009test\"]).mean()\n",
    "mean_2009_us = (mergedFrame[\"links_us2009\"] - mergedFrame[\"links_us2009test\"]).mean()\n",
    "\n",
    "mean_2019_own = (mergedFrame[\"links_own2019\"] - mergedFrame[\"links_own2019test\"]).mean()\n",
    "mean_2019_us = (mergedFrame[\"links_us2019\"] - mergedFrame[\"links_us2019test\"]).mean()\n",
    "\n",
    "print(\"2009 own: {}\".format(mean_2009_own))\n",
    "print(\"2009 US: {}\".format(mean_2009_us))\n",
    "print(\"2019 own: {}\".format(mean_2019_own))\n",
    "print(\"2019 US: {}\".format(mean_2019_us))\n",
    "\n",
    "# test_cols = [title for title in mergedFrame.columns if \"test\" in title]\n",
    "# for title in test_cols:\n",
    "#     del mergedFrame[title]\n",
    "\n",
    "### Part 2.2 - Getting article length in number of sentances\n",
    "\n",
    "# Steps/process:\n",
    "# - What we want to tidy up is any place where standardised by Wikipedia\n",
    "# - With wiki parser\n",
    "#  - Remove internal links which are lists, categories or languages BUT not Images (as contain text)\n",
    "#  - Remove navigation elements (templates) as same across categories BUT not columns (as contain text)\n",
    "#  - Remove tags as these these were not handled effectively\n",
    "#  - Split Image links manually (as wmparserfromhell has issues)\n",
    "#  - Clean out the columns so that we just get text\n",
    "#  - automated get display text on page -- removal of all other elements (external links, references, html elements etc)\n",
    "\n",
    "### Define some functions to clean up the data\n",
    "\n",
    "## Define what needs to be removed\n",
    "def getCleaning(wikicode):\n",
    "    templates = wikicode.filter_templates() \n",
    "    templates = [template for template in templates if \"column\" not in template]\n",
    "    \n",
    "    tags = wikicode.filter_tags() #remove <ref></ref>\n",
    "\n",
    "    int_links = wikicode.filter_wikilinks()\n",
    "    int_links_bad = [link for link in int_links if ':' in link and 'Image' not in link] #Remove everything thats not an image\n",
    "    int_links_bad +=  [link for link in int_links if 'List' in link] #remove links to lists\n",
    "    \n",
    "    to_clean = templates +int_links_bad + tags\n",
    "    return to_clean\n",
    "\n",
    "## Get the text from columns and images then strip everything \n",
    "def tidyPage(clean_wikicode):\n",
    "    new_int_links = clean_wikicode.filter_wikilinks()      #clean the links\n",
    "    new_int_links = set([str(link) for link in new_int_links])\n",
    "    for link in new_int_links:\n",
    "        if \"Image\" in link:   #get the display text out of the image wrapper\n",
    "            splitimage = link.split(\"|\")\n",
    "            imagetext = splitimage[len(splitimage)-1]\n",
    "            imagetext = re.sub(\"]]$\",\"\",imagetext).strip()\n",
    "            try:\n",
    "                clean_wikicode.replace(link,str(imagetext))\n",
    "            except:\n",
    "                pass\n",
    "                print(\"Error with image: {}\".format(imagetext))     #Catches images with no text\n",
    "\n",
    "    new_templates = clean_wikicode.filter_templates()\n",
    "    for column in new_templates:  #get the text out of the columns in a table wrapper\n",
    "        col = re.sub(\"\\n\",\"\",str(column))\n",
    "        splitcols = col.split(\"|col\")\n",
    "        splitcols = splitcols[1:]\n",
    "        splitcols = [col.split(\"=\",1)[1] for col in splitcols]\n",
    "        colphrase = ' '.join(splitcols)\n",
    "        try:\n",
    "            clean_wikicode.replace(str(column),splitcols)\n",
    "        except:\n",
    "            pass\n",
    "            print(\"Error in columns\")\n",
    "    \n",
    "    output_code = clean_wikicode.strip_code()\n",
    "    return output_code\n",
    "\n",
    "## Run all of this to clear out the gubbins\n",
    "def cleanPage(page):\n",
    "    wikipage = mwp.parse(page,skip_style_tags=True)\n",
    "    obj_to_remove = getCleaning(wikipage)   \n",
    "    \n",
    "    for item in obj_to_remove:\n",
    "        try:\n",
    "            wikipage.remove(item)\n",
    "        except:\n",
    "            pass #when item has already been removed\n",
    "        \n",
    "    clean_wikicode = tidyPage(wikipage)  \n",
    "    return clean_wikicode\n",
    "\n",
    "#Apply cleaning\n",
    "mergedFrame[\"clean_text2019\"] = mergedFrame[\"text2019\"].map(lambda x: cleanPage(x)) \n",
    "mergedFrame[\"clean_text2009\"] = mergedFrame[\"text2009\"].map(lambda x: cleanPage(x))\n",
    "\n",
    "##remove numbers with decimal place in between\n",
    "for year in bothyears:\n",
    "    mergedFrame['clean_text{}'.format(year)] = mergedFrame['clean_text{}'.format(year)].map(lambda x: re.sub(\"[0-9]\\.[0-9]\",\",\",x))\n",
    "\n",
    "##get number of sentances \n",
    "for year in bothyears:\n",
    "    mergedFrame['sent_length{}'.format(year)] = mergedFrame['clean_text2{}'.format(year)].map(lambda x: len(re.compile(r\"[A-Z][^\\.!?]*[\\.!?]\").findall(x)))\n",
    "\n",
    "## Test sentence regex on random articles\n",
    "import random\n",
    "rand_articles = [random.randint(1,196) for x in range(10)]\n",
    "\n",
    "for c,art in enumerate(rand_articles):\n",
    "    if c % 2 == 0:\n",
    "        year = \"2009\"\n",
    "    else:\n",
    "        year = \"2019\"\n",
    "    display(mergedFrame[\"clean_text{}\".format(year)][art])\n",
    "    \n",
    "### Part 2.3 - prepping for analysis\n",
    "\n",
    "#Exclude short articles and US\n",
    "mergedFrame[\"exclude\"] = ((mergedFrame[\"links_us2009\"] == 0) & (mergedFrame[\"sent_length2009\"] < 10)) | (mergedFrame[\"country\"]==\"United States\")\n",
    "\n",
    "mergedFrame.to_csv('mergedFrame.csv')\n",
    "\n",
    "#### Part 3 - analysis\n",
    "\n",
    "#new libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "from scipy import stats\n",
    "\n",
    "#Open CSV\n",
    "merged_df = pd.read_csv('mergedFrame.csv',index_col=0)\n",
    "# display(mergedFrame.head())\n",
    "\n",
    "valid_merged = merged_df[merged_df[\"exclude\"]==False]\n",
    "# print(len(valid_merged))\n",
    "\n",
    "### Part 3.1 - link analysis\n",
    "#link descriptives\n",
    "link_desc_df = valid_merged[[name for name in valid_merged.columns if \"links_us\" in name]].describe()\n",
    "display(link_desc_df)\n",
    "link_desc_df.to_csv(\"link_descriptives.csv\")\n",
    "\n",
    "#link t-test\n",
    "link_ttest = stats.ttest_rel(valid_merged[\"links_us2019\"],valid_merged[\"links_us2009\"])\n",
    "link_ttest_outputs = [{\"type\":\"link\",\"test\":link_ttest[0],\"pvalue\":link_ttest[1]}]\n",
    "link_ttest_df = pd.DataFrame(link_ttest_outputs)\n",
    "link_ttest_df.to_csv(\"link_ttest.csv\")\n",
    "display(link_ttest_df)\n",
    "\n",
    "#link plot\n",
    "link_plot = sns.distplot(valid_merged[\"links_us2009\"],color=\"red\",label=\"2009\")\n",
    "# plt.show()\n",
    "sns.distplot(valid_merged[\"links_us2019\"],color=\"blue\",label=\"2019\")\n",
    "# plt.show()\n",
    "\n",
    "link_plot.set_title(\"Number of links to US top-level domains\")\n",
    "link_plot.set(xlabel=\"Links to US sources\",ylabel=\"Density\")\n",
    "link_plot.legend()\n",
    "# plt.xlim(right= 0.035)\n",
    "fig1 = link_plot.get_figure()\n",
    "fig1.savefig(fname =\"link_plot.png\",dpi =500)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### Part 3.2 - sentence analysis\n",
    "\n",
    "#article descriptives\n",
    "art_desc_df = valid_merged[[name for name in valid_merged.columns if \"sent_length\" in name]].describe()\n",
    "display(art_desc_df)\n",
    "art_desc_df.to_csv(\"art_descriptives.csv\")\n",
    "\n",
    "#article t-test\n",
    "art_ttest = stats.ttest_rel(valid_merged[\"sent_length2019\"],valid_merged[\"sent_length2009\"])\n",
    "art_ttest_outputs = [{\"type\":\"sent length\",\"test\":art_ttest[0],\"pvalue\":art_ttest[1]}]\n",
    "art_ttest_df = pd.DataFrame(art_ttest_outputs)\n",
    "art_ttest_df.to_csv(\"art_ttest_outputs.csv\")\n",
    "display(art_ttest_df)\n",
    "\n",
    "#plot for article\n",
    "art_plot = sns.distplot(valid_merged[\"sent_length2009\"],color=\"red\",label=\"2009\") #,kde=False\n",
    "sns.distplot(valid_merged[\"sent_length2019\"],color=\"blue\",label=\"2019\")\n",
    "\n",
    "art_plot.set_title(\"Number of sentences per article\")\n",
    "art_plot.set(xlabel=\"Number of sentences\",ylabel=\"Density\")\n",
    "art_plot.legend()\n",
    "fig1 = art_plot.get_figure()\n",
    "fig1.savefig(fname =\"art_plot.png\",dpi =500)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#article correlation\n",
    "bothyears = [\"2009\",\"2019\"]\n",
    "# bothlinks = [\"links_us\"] #\"links_own\",\n",
    "correl_list = []\n",
    "\n",
    "for year in bothyears:\n",
    "    tempdic = {}\n",
    "    pearson = stats.pearsonr(x = valid_merged[\"links_us{}\".format(year)], y = valid_merged[\"sent_length{}\".format(year)])\n",
    "    tempdic[\"pearson_test\"] = pearson[0]\n",
    "    tempdic[\"pearson_pvalue\"] = pearson[1]\n",
    "    temp_df = pd.DataFrame([tempdic.values()],index=[year] ,columns=tempdic.keys())\n",
    "    correl_list.append(temp_df)\n",
    "    sns.jointplot(x = valid_merged[\"links_us{}\".format(year)], y = valid_merged[\"sent_length{}\".format(year)])\n",
    "    plt.show()\n",
    "correl_link_df = pd.concat(correl_list,axis=0)\n",
    "correl_link_df.to_csv(\"art_correlation.csv\")\n",
    "display(correl_link_df)\n",
    "\n",
    "### Part 3.3 - link per sentence analysis\n",
    "\n",
    "#links per sentence columns\n",
    "bothyears = [\"2009\",\"2019\"]\n",
    "for year in bothyears:\n",
    "    valid_merged[\"lps_us{}\".format(year)] = valid_merged.apply(lambda x: x.loc[\"links_us{}\".format(year)]/x.loc[\"sent_length{}\".format(year)],axis=1)\n",
    "\n",
    "#descriptives for lps\n",
    "lps_desc_df = valid_merged[[name for name in valid_merged.columns if \"lps_us\" in name]].describe()\n",
    "display(lps_desc_df)\n",
    "lps_desc_df.to_csv(\"lps_descriptives.csv\")\n",
    "\n",
    "#t-test for lps\n",
    "lps_ttest = stats.ttest_rel(valid_merged[\"lps_us2019\"],valid_merged[\"lps_us2009\"])\n",
    "lps_ttest_outputs = [{\"type\":\"lps\",\"test\":lps_ttest[0],\"pvalue\":lps_ttest[1]}]\n",
    "lps_ttest_df = pd.DataFrame(lps_ttest_outputs)\n",
    "\n",
    "lps_ttest_df.to_csv(\"lps_ttest_outputs.csv\")\n",
    "display(lps_ttest_df)\n",
    "\n",
    "#plot for lps\n",
    "lps_plot = sns.distplot(valid_merged[\"lps_us2009\"],color=\"red\",label=\"2009\") #,kde=False\n",
    "sns.distplot(valid_merged[\"lps_us2019\"],color=\"blue\",label=\"2019\")\n",
    "\n",
    "lps_plot.set_title(\"Links to US top-level domains per sentence in article\")\n",
    "lps_plot.set(xlabel=\"Links to US sources per sentence in article\",ylabel=\"Density\")\n",
    "lps_plot.legend()\n",
    "plt.ylim(top= 37)\n",
    "fig1 = lps_plot.get_figure()\n",
    "fig1.savefig(fname =\"lps_plot.png\",dpi =500)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
